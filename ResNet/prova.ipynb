{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.check_output([\"start\",\"./onbpr4\"],shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def trace_inner_product(a, b):\n",
    "           return not (a == 0 or b == 0 or a == b)\n",
    "class FileReader:\n",
    "    def __init__(self, n, k, m, code_type, trained):\n",
    "        self.mycodetype = code_type\n",
    "        self.N = n\n",
    "        self.K = k\n",
    "        self.M = m\n",
    "        self.G_rows = n + k\n",
    "        self.mTrained = trained\n",
    "        self.maxDv = 0\n",
    "        self.maxDc = 0\n",
    "        self.dv = []\n",
    "        self.dc = []\n",
    "        self.Nvk = []\n",
    "        self.Mck = []\n",
    "        self.Nv = []\n",
    "        self.Mc = []\n",
    "        self.checkVal = []\n",
    "        self.varVal = []\n",
    "        self.G = []\n",
    "        self.weights_cn = []\n",
    "        self.weights_ri = []\n",
    "        self.weights_llr = []\n",
    "        self.weights_vn = []\n",
    "        self.trained_iter = 0\n",
    "        self.read_H()\n",
    "        self.read_G()\n",
    "        if trained:\n",
    "            self.load_cn_weights()\n",
    "            self.load_llr_weights()\n",
    "            self.load_vn_weights()\n",
    "            self.load_ri_weights()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def read_H(self):\n",
    "        code_type_string = self.code_type_string()\n",
    "        filename = f\"./PCMs/{code_type_string}_{self.N}_{self.K}/{code_type_string}_{self.N}_{self.K}_H_{self.M}.alist\"\n",
    "\n",
    "        checkValues = []\n",
    "        VariableValues = []\n",
    "\n",
    "        with open(filename, 'r') as matrix_file:\n",
    "            # First line: n k\n",
    "            n, m = map(int, matrix_file.readline().split())\n",
    "            if n != self.N:\n",
    "                raise RuntimeError(\"read_H: file-specified N not as expected\")\n",
    "            if m != self.M:\n",
    "                raise RuntimeError(\"read_H: file-specified M not as expected\")\n",
    "\n",
    "            # Second line: max dv and dc\n",
    "            maxDv, maxDc = map(int, matrix_file.readline().split())\n",
    "\n",
    "            # Third line: dv degrees\n",
    "            dv = list(map(int, matrix_file.readline().split()))\n",
    "\n",
    "            # Fourth line: dc degrees\n",
    "            dc = list(map(int, matrix_file.readline().split()))\n",
    "\n",
    "            Nvk = [[] for _ in range(n)]\n",
    "            Mck = [[] for _ in range(m)]\n",
    "\n",
    "            # Fifth to n+5-th line: dv neighbors\n",
    "            Nv = []\n",
    "            for i in range(n):\n",
    "                line = list(map(int, matrix_file.readline().split()))\n",
    "                Nv.append([0] * dv[i])\n",
    "                for j in range(dv[i]):\n",
    "                    Nv[i][j] = line[j] - 1\n",
    "                    Mck[Nv[i][j]].append(j)\n",
    "\n",
    "            # n+6-th to n+6+m-th line: dc neighbors\n",
    "            Mc = []\n",
    "            for i in range(m):\n",
    "                line = list(map(int, matrix_file.readline().split()))\n",
    "                Mc.append([0] * dc[i])\n",
    "                \n",
    "                for j in range(dc[i]):\n",
    "                    Mc[i][j] = line[j] - 1\n",
    "                    \n",
    "                    Nvk[Mc[i][j]].append(j)\n",
    "\n",
    "            # n+6+m+1-th to n+6+m+n-th line: value of each row\n",
    "            for i in range(m):\n",
    "                line = list(map(int, matrix_file.readline().split()))\n",
    "                #while line[-1] == 0:\n",
    "                  # line.pop()\n",
    "                checkValues.append(line)\n",
    "                \n",
    "\n",
    "            # Last lines: value of each column\n",
    "            for i in range(n):\n",
    "                line = list(map(int, matrix_file.readline().split()))\n",
    "                VariableValues.append(line)\n",
    "        self.maxDv = maxDv\n",
    "        self.maxDc = maxDc\n",
    "        self.dv = dv\n",
    "        self.dc = dc\n",
    "        self.Nvk = Nvk\n",
    "        self.Mck = Mck\n",
    "        self.Nv = Nv\n",
    "        self.Mc = Mc\n",
    "        self.checkVal = checkValues\n",
    "        self.varVal = VariableValues\n",
    "        \n",
    "        print(\"Length of dv:\", len(self.dv))\n",
    "        print(\"Length of dc:\", len(self.dc))\n",
    "        print(\"Length of Nvk:\", len(self.Nvk))\n",
    "        print(\"Length of Mck:\", len(self.Mck))\n",
    "        print(\"Length of Nv:\", len(self.Nv))\n",
    "        print(\"Length of Mc:\", len(self.Mc))\n",
    "        print(\"Length of checkVal:\", len(self.checkVal))\n",
    "        print(\"Length of varVal:\", len(self.varVal))\n",
    "        print(len(self.Nvk))\n",
    "        \n",
    "    def save_values_to_file(self):\n",
    "        with open(\"values.txt\", \"w\") as file:\n",
    "            file.write(\"maxDv: \" + str(self.maxDv) + \"\\n\")\n",
    "            file.write(\"maxDc: \" + str(self.maxDc) + \"\\n\")\n",
    "            file.write(\"dv: \" + str(self.dv) + \"\\n\")\n",
    "            file.write(\"dc: \" + str(self.dc) + \"\\n\")\n",
    "            file.write(\"Nvk: \" + str(self.Nvk) + \"\\n\")\n",
    "            file.write(\"Mck: \" + str(self.Mck) + \"\\n\")\n",
    "            file.write(\"Nv: \" + str(self.Nv) + \"\\n\")\n",
    "            file.write(\"Mc: \" + str(self.Mc) + \"\\n\")\n",
    "            file.write(\"checkVal: \" + str(self.checkVal) + \"\\n\")\n",
    "            file.write(\"varVal: \" + str(self.varVal) + \"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def read_Mck_from_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "             lines = file.readlines()\n",
    "        # Assuming each line in Mck.txt contains a list of integers separated by whitespace\n",
    "             Mck_from_file = [list(map(int, line.strip().split())) for line in lines]\n",
    "        return Mck_from_file\n",
    "\n",
    "    def compare_Mck_with_file(self, filename):\n",
    "        Mck_from_file = self.read_Mck_from_file(filename)\n",
    "    \n",
    "        if len(Mck_from_file) != len(self.Mck):\n",
    "           print(\"Length of Mck does not match the length of Mck from file.\")\n",
    "           return False\n",
    "    \n",
    "        for i in range(len(self.Mck)):\n",
    "            if len(Mck_from_file[i]) != len(self.Mck[i]):\n",
    "                print(f\"Length of Mck[{i}] does not match the length of Mck[{i}] from file.\")\n",
    "                return False\n",
    "        \n",
    "            for j in range(len(self.Mck[i])):\n",
    "               if Mck_from_file[i][j] != self.Mck[i][j]:\n",
    "                   print(f\"Mismatch found in Mck[{i}][{j}]: Expected {Mck_from_file[i][j]}, Got {self.Mck[i][j]}\")\n",
    "                   return False\n",
    "            \n",
    "        print(\"Mck matches the values from file.\")\n",
    "        return True\n",
    "    \n",
    "    def read_Nvk_from_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "             lines = file.readlines()\n",
    "        # Assuming each line in Mck.txt contains a list of integers separated by whitespace\n",
    "             Nvk_from_file = [list(map(int, line.strip().split())) for line in lines]\n",
    "        return Nvk_from_file\n",
    "    \n",
    "    \n",
    "    def compare_Nvk_with_file(self, filename):\n",
    "        Nvk_from_file = self.read_Nvk_from_file(filename)\n",
    "    \n",
    "        if len(Nvk_from_file) != len(self.Nvk):\n",
    "           print(\"Length of Nvk does not match the length of Nvkk from file.\")\n",
    "           return False\n",
    "    \n",
    "        for i in range(len(self.Nvk)):\n",
    "            if len(Nvk_from_file[i]) != len(self.Nvk[i]):\n",
    "                print(f\"Length of Nvk[{i}] does not match the length of Nvk[{i}] from file.\")\n",
    "                return False\n",
    "        \n",
    "            for j in range(len(self.Nvk[i])):\n",
    "               if Nvk_from_file[i][j] != self.Nvk[i][j]:\n",
    "                   print(f\"Mismatch found in Nvk[{i}][{j}]: Expected {Nvk_from_file[i][j]}, Got {self.Nvk[i][j]}\")\n",
    "                   return False\n",
    "            \n",
    "        print(\"Nvk matches the values from file.\")\n",
    "        return True\n",
    "    \n",
    "    def read_Mc_from_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "             lines = file.readlines()\n",
    "        # Assuming each line in Mck.txt contains a list of integers separated by whitespace\n",
    "             Mc_from_file = [list(map(int, line.strip().split())) for line in lines]\n",
    "        return Mc_from_file\n",
    "    \n",
    "    def compare_Mc_with_file(self, filename):\n",
    "        Mc_from_file = self.read_Mc_from_file(filename)\n",
    "    \n",
    "        if len(Mc_from_file) != len(self.Mc):\n",
    "           print(\"Length of Mc does not match the length of Nvkk from file.\")\n",
    "           return False\n",
    "    \n",
    "        for i in range(len(self.Mc)):\n",
    "            if len(Mc_from_file[i]) != len(self.Mc[i]):\n",
    "                print(f\"Length of Mc[{i}] does not match the length of Mc[{i}] from file.\")\n",
    "                return False\n",
    "        \n",
    "            for j in range(len(self.Mc[i])):\n",
    "               if Mc_from_file[i][j] != self.Mc[i][j]:\n",
    "                   print(f\"Mismatch found in Mc[{i}][{j}]: Expected {Mc_from_file[i][j]}, Got {self.Mc[i][j]}\")\n",
    "                   return False\n",
    "            \n",
    "        print(\"Mc matches the values from file.\")\n",
    "        return True\n",
    "    \n",
    "    def read_Nv_from_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "             lines = file.readlines()\n",
    "        # Assuming each line in Mck.txt contains a list of integers separated by whitespace\n",
    "             Nv_from_file = [list(map(int, line.strip().split())) for line in lines]\n",
    "        return Nv_from_file\n",
    "    \n",
    "    def compare_Nv_with_file(self, filename):\n",
    "        Nv_from_file = self.read_Nv_from_file(filename)\n",
    "    \n",
    "        if len(Nv_from_file) != len(self.Nv):\n",
    "           print(\"Length of Mc does not match the length of Nvkk from file.\")\n",
    "           return False\n",
    "    \n",
    "        for i in range(len(self.Nv)):\n",
    "            if len(Nv_from_file[i]) != len(self.Nv[i]):\n",
    "                print(f\"Length of Nv[{i}] does not match the length of Nv[{i}] from file.\")\n",
    "                return False\n",
    "        \n",
    "            for j in range(len(self.Mc[i])):\n",
    "               if Nv_from_file[i][j] != self.Nv[i][j]:\n",
    "                   print(f\"Mismatch found in Nv[{i}][{j}]: Expected {Nv_from_file[i][j]}, Got {self.Nv[i][j]}\")\n",
    "                   return False\n",
    "            \n",
    "        print(\"Nv matches the values from file.\")\n",
    "        return True\n",
    "    \n",
    "    def read_checkVal_from_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "             lines = file.readlines()\n",
    "        # Assuming each line in Mck.txt contains a list of integers separated by whitespace\n",
    "             checkVal_from_file = [list(map(int, line.strip().split())) for line in lines]\n",
    "        return checkVal_from_file\n",
    "    \n",
    "    def compare_checkVal_with_file(self, filename):\n",
    "        checkVal_from_file = self.read_checkVal_from_file(filename)\n",
    "    \n",
    "        if len(checkVal_from_file) != len(self.checkVal):\n",
    "           print(\"Length of checkVal does not match the length of checkVal from file.\")\n",
    "           return False\n",
    "    \n",
    "        for i in range(len(self.checkVal)):\n",
    "            if len(checkVal_from_file[i]) != len(self.checkVal[i]):\n",
    "                print(f\"Length of checkVal[{i}] does not match the length of checkVal[{i}] from file.\")\n",
    "                return False\n",
    "        \n",
    "            for j in range(len(self.checkVal[i])):\n",
    "               if checkVal_from_file[i][j] != self.checkVal[i][j]:\n",
    "                   print(f\"Mismatch found in checkVal[{i}][{j}]: Expected {checkVal_from_file[i][j]}, Got {self.checkVal[i][j]}\")\n",
    "                   return False\n",
    "            \n",
    "        print(\"checkVal matches the values from file.\")\n",
    "        return True\n",
    "    \n",
    "    def read_varVal_from_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "             lines = file.readlines()\n",
    "        # Assuming each line in Mck.txt contains a list of integers separated by whitespace\n",
    "             varVal_from_file = [list(map(int, line.strip().split())) for line in lines]\n",
    "        return varVal_from_file\n",
    "    \n",
    "    \n",
    "    def compare_varVal_with_file(self, filename):\n",
    "        varVal_from_file = self.read_varVal_from_file(filename)\n",
    "    \n",
    "        if len(varVal_from_file) != len(self.varVal):\n",
    "           print(\"Length of varVal does not match the length of varVal from file.\")\n",
    "           return False\n",
    "    \n",
    "        for i in range(len(self.varVal)):\n",
    "            if len(varVal_from_file[i]) != len(self.varVal[i]):\n",
    "                print(f\"Length of varVal[{i}] does not match the length of varVal[{i}] from file.\")\n",
    "                return False\n",
    "        \n",
    "            for j in range(len(self.varVal[i])):\n",
    "               if varVal_from_file[i][j] != self.varVal[i][j]:\n",
    "                   print(f\"Mismatch found in varVal[{i}][{j}]: Expected {varVal_from_file[i][j]}, Got {self.varVal[i][j]}\")\n",
    "                   return False\n",
    "            \n",
    "        print(\"varVal matches the values from file.\")\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    \n",
    "    def read_G(self):\n",
    "        code_type_string = self.code_type_string()\n",
    "        filename = f\"./PCMs/{code_type_string}_{self.N}_{self.K}/{code_type_string}_{self.N}_{self.K}_G.txt\"\n",
    "        with open(filename, 'r') as matrix_file:\n",
    "            for _ in range(self.G_rows):\n",
    "                row = list(map(int, matrix_file.readline().split()))\n",
    "                self.G.append(row)\n",
    "        print(self.G)\n",
    "\n",
    "    def load_cn_weights(self):\n",
    "        path = self.construct_weights_path(\"weight_cn.txt\")\n",
    "        weight_cn = []\n",
    "        with open(path, 'r') as weights_file:\n",
    "            dec_iter = int(weights_file.readline())\n",
    "            self.trained_iter = dec_iter\n",
    "            n, m = map(int, weights_file.readline().split())\n",
    "            assert n == self.N, \"load_cn_weights: file-specified N not as expected\"\n",
    "            assert m == self.M, \"load_cn_weights: file-specified M not as expected\"\n",
    "            for _ in range(dec_iter):\n",
    "                weight_cn_tmp = []\n",
    "                for _ in range(m):\n",
    "                    weight_cn_row = list(map(float, weights_file.readline().split()))\n",
    "                    weight_cn_tmp.append(weight_cn_row)\n",
    "                weight_cn.append(weight_cn_tmp)\n",
    "        self.weights_cn = weight_cn\n",
    "        \n",
    "    def load_ri_weights(self):\n",
    "        path = self.construct_weights_path(\"weight_ri.txt\")\n",
    "        weight_ri = []\n",
    "        with open(path, 'r') as weights_file:\n",
    "            dec_iter = int(weights_file.readline())\n",
    "            self.trained_iter = dec_iter\n",
    "            n, m = map(int, weights_file.readline().split())\n",
    "            assert n == self.N, \"load_ri_weights: file-specified N not as expected\"\n",
    "            assert m == self.M, \"load_ri_weights: file-specified M not as expected\"\n",
    "            for _ in range(dec_iter):\n",
    "                weight_ri_tmp = []\n",
    "                for _ in range(m):\n",
    "                    weight_ri_row = list(map(float, weights_file.readline().split()))\n",
    "                    weight_ri_tmp.append(weight_ri_row)\n",
    "                weight_ri.append(weight_ri_tmp)\n",
    "        self.weights_ri = weight_ri\n",
    "\n",
    "    def load_llr_weights(self):\n",
    "        path = self.construct_weights_path(\"weight_llr.txt\")\n",
    "        weight_llr = []\n",
    "        with open(path, 'r') as weights_file:\n",
    "            dec_iter = int(weights_file.readline())\n",
    "            for _ in range(dec_iter):\n",
    "                weight_llr.append(list(map(float, weights_file.readline().split())))\n",
    "        self.weights_llr = weight_llr\n",
    "\n",
    "    def load_vn_weights(self):\n",
    "        path = self.construct_weights_path(\"weight_vn.txt\")\n",
    "        weight_vn = []\n",
    "        with open(path, 'r') as weights_file:\n",
    "            dec_iter = int(weights_file.readline())\n",
    "            n, m = map(int, weights_file.readline().split())\n",
    "            assert n == self.N, \"load_vn_weights: file-specified N not as expected\"\n",
    "            assert m == self.M, \"load_vn_weights: file-specified M not as expected\"\n",
    "            for _ in range(dec_iter):\n",
    "                weight_vn_tmp = []\n",
    "                for _ in range(n):\n",
    "                    weight_vn_row = list(map(float, weights_file.readline().split()))\n",
    "                    weight_vn_tmp.append(weight_vn_row)\n",
    "                weight_vn.append(weight_vn_tmp)\n",
    "        self.weights_vn = weight_vn\n",
    "\n",
    "    def construct_weights_path(self, filename):\n",
    "        code_type_string = self.code_type_string()\n",
    "        directory_name_builder = f\"{code_type_string}_{self.N}_{self.K}_{self.M}\"\n",
    "        path = os.path.join(\"training_results\", directory_name_builder, filename)\n",
    "        return path\n",
    "\n",
    "    def code_type_string(self):\n",
    "        if self.mycodetype == 'GB':\n",
    "            return \"GB\"\n",
    "        elif self.mycodetype == 'HP':\n",
    "            return \"HP\"\n",
    "        elif self.mycodetype == 'toric':\n",
    "            return \"toric\"\n",
    "        else:\n",
    "            raise ValueError(\"Unimplemented codetype\")\n",
    "    \n",
    "    #about this, the matrices are identical, probably there is an error in the code, but with c++ works so no problem?\\\n",
    "        \n",
    "    \n",
    "       \n",
    "    def check_symplectic(self):\n",
    "        vec1 = [0] * self.N\n",
    "        vec2 = [0] * self.N\n",
    "        \n",
    "        for rowid in range(self.M):\n",
    "                 for iii in range(self.N):\n",
    "                     print(\"ciao\")\n",
    "                     vec1[iii] = 0\n",
    "                 for j in range(len(self.Mc[rowid])):\n",
    "                     vec1[self.Mc[rowid][j]] = self.checkVal[rowid][j]\n",
    "                     print(vec1[j], end=\" \")\n",
    "\n",
    "                 for i in range(self.M):\n",
    "                    for iii in range(self.N):\n",
    "                       vec2[iii] = 0\n",
    "                    for j in range(len(self.Mc[i])):\n",
    "                       vec2[self.Mc[i][j]] = self.checkVal[i][j]\n",
    "                 syn_check = False\n",
    "                 for j in range(self.N):\n",
    "                    syn_check = not (vec1[j] == 0 or vec2[j] == 0 or vec1[j] == vec2[j])\n",
    "                    syn_check = not syn_check if syn_check else syn_check\n",
    "                 if syn_check:\n",
    "                     raise RuntimeError(\"check_symplectic: syn_check % 2 != 0\")\n",
    "                 \n",
    "                 for i in range(self.G_rows):\n",
    "                     syn_check = False\n",
    "                     for j in range(self.N):\n",
    "                         syn_check = not (vec1[j] == 0 or self.G[i][j] == 0 or vec1[j] == self.G[i][j])\n",
    "                         syn_check = not syn_check if syn_check else syn_check\n",
    "                     if syn_check:\n",
    "                         raise RuntimeError(\"check_symplectic / GH^T: syn_check % 2 != 0\")\n",
    "                #syn_check = False\n",
    "                #for j in range(self.N):\n",
    "                #   syn_check = self.trace_inner_product(vec1[j], vec2[j]) != 0\n",
    "                #   if syn_check:\n",
    "                #        raise ValueError(\"check_symplectic: syn_check % 2 != 0\")\n",
    "\n",
    "          #  for i in range(self.G_rows):\n",
    "           #     syn_check = False\n",
    "            #    for j in range(self.N):\n",
    "             #       syn_check = self.trace_inner_product(vec1[j], self.G[i][j]) != 0\n",
    "              #      if syn_check:\n",
    "               #         raise ValueError(\"check_symplectic / GH^T: syn_check % 2 != 0\")\n",
    "\n",
    "        print(\"Check symplectic ok\")\n",
    "        return True\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class StabilizerCodes:\n",
    "    def __init__(self, n, k, m, code_type, fr, trained):\n",
    "        self.mycodetype = code_type\n",
    "        self.N = n\n",
    "        self.K = k\n",
    "        self.M = m\n",
    "        self.trained_iter = 0  # Define trained_iter here\n",
    "        self.G_rows = self.N + self.K\n",
    "        self.mTrained = trained\n",
    "        self.dc = fr.dc\n",
    "        self.dv = fr.dv\n",
    "        self.maxDc = fr.maxDc\n",
    "        self.maxDv = fr.maxDv\n",
    "        self.Nv = fr.Nv\n",
    "        self.Mc = fr.Mc\n",
    "        self.checkVal = fr.checkVal\n",
    "        self.varVal = fr.varVal\n",
    "        self.Nvk = fr.Nvk\n",
    "        self.Mck = fr.Mck\n",
    "        self.G = fr.G\n",
    "        if trained:\n",
    "            self.weights_cn = fr.weights_cn\n",
    "            self.weights_vn = fr.weights_vn\n",
    "            self.weights_llr = fr.weights_llr\n",
    "\n",
    "    def decode(self, L, epsilon):\n",
    "        if not self.errorString:\n",
    "            return [True, True]\n",
    "\n",
    "        self.calculate_syndrome()\n",
    "        self.error_hat = np.zeros(self.N, dtype=int)\n",
    "        return self.flooding_decode(L, epsilon)\n",
    "\n",
    "    def add_error_given_epsilon(self, epsilon):\n",
    "        self.error = []\n",
    "        self.errorString = []\n",
    "        for i in range(self.N):\n",
    "            rndValue = np.random.rand()\n",
    "            if rndValue < epsilon / 3:\n",
    "                self.error.append(1)\n",
    "                self.errorString.append(\"X\" + str(i))\n",
    "            elif rndValue < epsilon * 2 / 3:\n",
    "                self.error.append(2)\n",
    "                self.errorString.append(\"Z\" + str(i))\n",
    "            elif rndValue < epsilon:\n",
    "                self.error.append(3)\n",
    "                self.errorString.append(\"Y\" + str(i))\n",
    "            else:\n",
    "                self.error.append(0)\n",
    "\n",
    "    def quantize_belief(self, Tau, Tau1, Tau2):\n",
    "        nom = np.log1p(np.exp(-1.0 * Tau))\n",
    "        denom = max(-1.0 * Tau1, -1.0 * Tau2) + np.log1p(np.exp(-1.0 * np.fabs((Tau1 - Tau2))))\n",
    "        ret_val = nom - denom\n",
    "        if np.isnan(ret_val):\n",
    "            raise RuntimeError(\"quantize_belief: Log difference is NaN\")\n",
    "        return ret_val\n",
    "\n",
    "    def trace_inner_product(self, a, b):\n",
    "        return not (a == 0 or b == 0 or a == b)\n",
    "\n",
    "    def flooding_decode(self, L, epsilon):\n",
    "        success = [False, False]\n",
    "        L0 = np.log(3.0 * (1 - epsilon) / epsilon)\n",
    "        lambda0 = np.log((1 + np.exp(-L0)) / (2 * np.exp(-L0)))\n",
    "\n",
    "        mc2v = np.zeros((self.M, max(self.dc)), dtype=float)\n",
    "        mv2c = np.zeros((self.N, max(self.dv)), dtype=float)\n",
    "        Taux = np.zeros(self.N, dtype=float)\n",
    "        Tauz = np.zeros(self.N, dtype=float)\n",
    "        Tauy = np.zeros(self.N, dtype=float)\n",
    "        phi_msg = np.zeros(max(self.dc), dtype=float)\n",
    "\n",
    "        for decIter in range(L):\n",
    "            for i in range(self.M):\n",
    "                phi_sum = 0\n",
    "                sign_prod = 1.0 if self.syn[i] == 0 else -1.0\n",
    "                for j in range(self.dc[i]):\n",
    "                    if mv2c[self.Mc[i][j]][self.Mck[i][j]] != 0.0:\n",
    "                        phi_msg[j] = -1.0 * np.log(np.tanh(np.fabs(mv2c[self.Mc[i][j]][self.Mck[i][j]]) / 2.0))\n",
    "                    else:\n",
    "                        phi_msg[j] = 60\n",
    "                    phi_sum += phi_msg[j]\n",
    "                    sign_prod *= 1.0 if mv2c[self.Mc[i][j]][self.Mck[i][j]] >= 0.0 else -1.0\n",
    "\n",
    "                for j in range(self.dc[i]):\n",
    "                    phi_extrinsic_phi_sum = phi_sum - phi_msg[j]\n",
    "                    phi_phi_sum = 60 if phi_extrinsic_phi_sum == 0 else -1.0 * np.log(np.tanh(phi_extrinsic_phi_sum / 2.0))\n",
    "                    mc2v[i][j] = phi_phi_sum * sign_prod * (1.0 if mv2c[self.Mc[i][j]][self.Mck[i][j]] >= 0.0 else -1.0)\n",
    "                    if self.mTrained and decIter < self.trained_iter:\n",
    "                        mc2v[i][j] *= self.weights_cn[decIter][i][j]\n",
    "                    if np.isnan(mc2v[i][j]):\n",
    "                        raise RuntimeError(\"flooding_decode: mc2v[i][j] is NaN\")\n",
    "                    if np.isinf(mc2v[i][j]):\n",
    "                        raise RuntimeError(\"flooding_decode: mc2v[i][j] is infinity\")\n",
    "\n",
    "            for Vidx in range(self.N):\n",
    "                Taux[Vidx] = L0\n",
    "                Tauz[Vidx] = L0\n",
    "                Tauy[Vidx] = L0\n",
    "                if self.mTrained and decIter < self.trained_iter:\n",
    "                    Taux[Vidx] *= self.weights_llr[decIter + 1][Vidx]\n",
    "                    Tauy[Vidx] *= self.weights_llr[decIter + 1][Vidx]\n",
    "                    Tauz[Vidx] *= self.weights_llr[decIter + 1][Vidx]\n",
    "\n",
    "                for jj in range(self.dv[Vidx]):\n",
    "                    if self.varVal[Vidx][jj] == 1:\n",
    "                        Tauz[Vidx] += mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        Tauy[Vidx] += mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                    elif self.varVal[Vidx][jj] == 2:\n",
    "                        Taux[Vidx] += mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        Tauy[Vidx] += mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                    elif self.varVal[Vidx][jj] == 3:\n",
    "                        Taux[Vidx] += mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        Tauz[Vidx] += mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                    else:\n",
    "                        raise ValueError(\"Something is wrong\")\n",
    "\n",
    "                for jj in range(self.dv[Vidx]):\n",
    "                    if self.varVal[Vidx][jj] == 1:\n",
    "                        Tauxi = Taux[Vidx]\n",
    "                        Tauzi = Tauz[Vidx] - mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        Tauyi = Tauy[Vidx] - mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        temp = self.quantize_belief(Tauxi, Tauyi, Tauzi)\n",
    "                    elif self.varVal[Vidx][jj] == 2:\n",
    "                        Tauxi = Taux[Vidx] - mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        Tauzi = Tauz[Vidx]\n",
    "                        Tauyi = Tauy[Vidx] - mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        temp = self.quantize_belief(Tauzi, Tauyi, Tauxi)\n",
    "                    elif self.varVal[Vidx][jj] == 3:\n",
    "                        Tauxi = Taux[Vidx] - mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        Tauzi = Tauz[Vidx] - mc2v[self.Nv[Vidx][jj]][self.Nvk[Vidx][jj]]\n",
    "                        Tauyi = Tauy[Vidx]\n",
    "                        temp = self.quantize_belief(Tauyi, Tauxi, Tauzi)\n",
    "                    else:\n",
    "                        raise ValueError(\"Something is wrong\")\n",
    "\n",
    "                    limit = 60\n",
    "                    if temp > limit:\n",
    "                        mv2c[Vidx][jj] = limit\n",
    "                    elif temp < -limit:\n",
    "                        mv2c[Vidx][jj] = -limit\n",
    "                    else:\n",
    "                        mv2c[Vidx][jj] = temp\n",
    "\n",
    "                    if self.mTrained and decIter < self.trained_iter:\n",
    "                        mv2c[Vidx][jj] *= self.weights_vn[decIter + 1][Vidx][jj]\n",
    "\n",
    "                    if np.isnan(mv2c[Vidx][jj]):\n",
    "                        raise RuntimeError(\"flooding_decode: mv2c[Vidx][jj] is NaN\")\n",
    "                    if np.isinf(mv2c[Vidx][jj]):\n",
    "                        raise RuntimeError(\"flooding_decode: mv2c[Vidx][jj] is infinity\")\n",
    "\n",
    "            success = self.check_success(Taux, Tauy, Tauz)\n",
    "            if success[0]:\n",
    "                break\n",
    "\n",
    "        return success\n",
    "\n",
    "    def calculate_syndrome(self):\n",
    "        self.syn = []\n",
    "        for i in range(self.M):\n",
    "            check = False\n",
    "            for j in range(self.dc[i]):\n",
    "                check = self.trace_inner_product(self.error[self.Mc[i][j]], self.checkVal[i][j])\n",
    "                check = not check if check else check\n",
    "            self.syn.append(check % 2)\n",
    "\n",
    "    def check_success(self, Taux, Tauy, Tauz):\n",
    "        success = [False, False]\n",
    "        self.error_hat = [0] * self.N\n",
    "        for i in range(self.N):\n",
    "            if Taux[i] > 0 and Tauy[i] > 0 and Tauz[i] > 0:\n",
    "                self.error_hat[i] = 0\n",
    "            elif Taux[i] < Tauy[i] and Taux[i] < Tauz[i]:\n",
    "                self.error_hat[i] = 1\n",
    "            elif Tauz[i] < Taux[i] and Tauz[i] < Tauy[i]:\n",
    "                self.error_hat[i] = 2\n",
    "            else:\n",
    "                self.error_hat[i] = 3\n",
    "\n",
    "        for i in range(self.M):\n",
    "            check = False\n",
    "            for j in range(self.dc[i]):\n",
    "                check = self.trace_inner_product(self.error_hat[self.Mc[i][j]], self.checkVal[i][j]) != check\n",
    "            if check != self.syn[i]:\n",
    "                return success\n",
    "\n",
    "        success[0] = True\n",
    "        for i in range(self.G_rows):\n",
    "            check = False\n",
    "            for j in range(self.N):\n",
    "                check = self.trace_inner_product(self.error[j], self.G[i][j]) != check\n",
    "                check = self.trace_inner_product(self.error_hat[j], self.G[i][j]) != check\n",
    "            if check:\n",
    "                return success\n",
    "\n",
    "        success[1] = True\n",
    "        return success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ParseResult:\n",
    "    def __init__(self):\n",
    "        self.epsilons = []\n",
    "        self.progname = \"\"\n",
    "        self.maximum_frame_errors = 30\n",
    "        self.maximum_decoded_words = 45000000\n",
    "        self.print_help = False\n",
    "\n",
    "def print_help(progname):\n",
    "    print(\n",
    "        \"Neural Belief Propagation Using Overcomplete Check Matrices\\n\"\n",
    "        \"\\n\"\n",
    "        \"Usage:\\n\"\n",
    "        f\"{progname} [options] [Epsilon] [Epsilon]...\\n\"\n",
    "        \"\\n\"\n",
    "        \"Positional arguments:\\n\"\n",
    "        \"  Epsilon                     optionally specify a list of epsilons at which to verify.\\n\"\n",
    "        \"                              If no epsilons are given, use default list of epsilons \\n\"\n",
    "        \"\\n\"\n",
    "        \"Options:\\n\"\n",
    "        \"  -r|--range START STEP STOP  Specify a decreasing range of epsilons to use, e.g.,\\n\"\n",
    "        \"                              \\\"-r 0.1 0.025 0.9\\\" to get 0.1 0.975 0.95 0.925 0.9\\n\"\n",
    "        \"\\n\"\n",
    "        \"  -e|--max-frame-errors N     Stop simulating after observing N frame errors\\n\"\n",
    "        \"  -w|--max-decoded-words N    Stop simulating after decoding N words\\n\"\n",
    "        \"\\n\\n\\n\"\n",
    "        \" This program accompanies the paper\\n\"\n",
    "        \"     S. Miao, A. Schnerring, H. Li and L. Schmalen,\\n\"\n",
    "        \"     \\\"Neural belief propagation decoding of quantum LDPC codes using overcomplete check matrices,\\\"\\n\"\n",
    "        \"     Proc. IEEE Inform. Theory Workshop (ITW), Saint-Malo, France, Apr. 2023, \"\n",
    "        \"https://arxiv.org/abs/2212.10245\\n\"\n",
    "    )\n",
    "\n",
    "def parse_arguments(argument_count, arguments, default_epsilons, maximum_frame_errors=30, maximum_decoded_words=45000000):\n",
    "    result = ParseResult()\n",
    "    result.maximum_frame_errors = maximum_frame_errors\n",
    "    result.maximum_decoded_words = maximum_decoded_words\n",
    "    if arguments:  # Check if arguments list is not empty\n",
    "        result.progname = arguments[0]\n",
    "        arguments = arguments[1:]\n",
    "        argument_count -= 1\n",
    "    else:\n",
    "        result.progname = \"default_progname\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    if not argument_count:  # no arguments passed – use default epsilons\n",
    "        result.epsilons = default_epsilons\n",
    "        return result  # return empty list\n",
    "\n",
    "    while argument_count:\n",
    "        argument = arguments[0]\n",
    "        argument_view = argument\n",
    "\n",
    "        if argument_view in (\"-h\", \"--help\"):\n",
    "            result.print_help = True\n",
    "            return result\n",
    "        if argument_view in (\"-r\", \"--range\"):\n",
    "            if argument_count < 4:  # need three more arguments\n",
    "                result.print_help = True\n",
    "                return result\n",
    "            start = float(arguments[1])\n",
    "            step = float(arguments[2])\n",
    "            stop = float(arguments[3])\n",
    "            argument_count -= 3\n",
    "            try:\n",
    "                if stop <= 0.0:\n",
    "                    raise ValueError(\"Stop value needs to be positive\")\n",
    "                if step <= 0.0:\n",
    "                    raise ValueError(\"step value needs to be positive\")\n",
    "                if start <= stop:\n",
    "                    raise ValueError(\"Stop value needs to be smaller than start value\")\n",
    "                result.epsilons = list(np.arange(start, stop, step)[::-1])\n",
    "            except ValueError as exceptions:\n",
    "                print(f\"Could not parse range: {exceptions}; skipping range.\")\n",
    "        elif argument_view in (\"-e\", \"--max-frame-errors\"):\n",
    "            argument_count -= 1\n",
    "            result.maximum_frame_errors = int(arguments[1])\n",
    "        elif argument_view in (\"-w\", \"--max-decoded-words\"):\n",
    "            argument_count -= 1\n",
    "            result.maximum_decoded_words = int(arguments[1])\n",
    "        else:\n",
    "            # we encountered something that doesn't look like a keyword argument -- proceed to parse numbers\n",
    "            break\n",
    "        arguments = arguments[1:]\n",
    "        argument_count -= 1\n",
    "\n",
    "    if not argument_count and not result.epsilons:  # no arguments passed, and no range found – use default epsilons\n",
    "        result.epsilons = default_epsilons\n",
    "        return result\n",
    "\n",
    "    result.epsilons = []\n",
    "    for argument in arguments:\n",
    "        value = float(argument)\n",
    "        if value <= 0.0:  # couldn't parse number, or actually 0.0 (or below) was passed\n",
    "            print(f\"Ignoring argument '{argument}' as unparseable or non-positive.\")\n",
    "            continue  # skip this\n",
    "        result.epsilons.append(value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dv: 32\n",
      "Length of dc: 96\n",
      "Length of Nvk: 32\n",
      "Length of Mck: 96\n",
      "Length of Nv: 32\n",
      "Length of Mc: 96\n",
      "Length of checkVal: 96\n",
      "Length of varVal: 32\n",
      "32\n",
      "[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0], [0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]\n"
     ]
    }
   ],
   "source": [
    "n = 32\n",
    "k = 2\n",
    "m = 96\n",
    "trained_iter=18\n",
    "decIterNum = 18\n",
    "trained = True\n",
    "ep0 = 0.37\n",
    "codeType = \"toric\"\n",
    "\n",
    "matrix_supplier = FileReader(n, k, m, codeType, trained)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "1 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 1 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 1 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 1 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "1 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 1 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 1 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 1 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "1 1 0 0 1 1 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "1 0 0 1 1 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "1 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 1 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 1 1 0 0 1 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 1 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 1 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 1 1 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 1 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 1 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 1 1 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 1 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 1 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 1 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 1 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "1 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 1 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 1 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 1 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "1 1 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "1 0 0 1 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 1 1 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 1 1 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "2 2 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 2 2 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 2 2 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "2 0 0 2 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "2 0 2 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 2 0 2 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "2 2 0 0 2 2 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "2 2 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 2 0 2 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 2 2 0 0 2 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 2 2 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "2 0 2 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 2 2 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 2 2 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "2 0 0 2 2 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "2 0 0 2 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 2 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 2 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 2 2 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 2 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 2 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 2 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 2 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "ciao\n",
      "0 0 0 0 0 0 Check symplectic ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_supplier.check_symplectic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROVA1\n",
      "Length of dv: 32\n",
      "Length of dc: 96\n",
      "Length of Nvk: 32\n",
      "Length of Mck: 96\n",
      "Length of Nv: 32\n",
      "Length of Mc: 96\n",
      "Length of checkVal: 96\n",
      "Length of varVal: 32\n",
      "32\n",
      "[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0], [0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]\n",
      "Check symplectic ok\n",
      "1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 2 2 0 0 0 2 2 0 0 0 2 2 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 2 0 2 0 0 2 2 0 0 2 2 2 2 0 0 0 0 0 2 0 2 0 0 0 2 2 0 0 2 0 2 2 0 0 0 2 0 2 0 0 0 0 0 2 2 0 0 0 0 2 2 0 0 2 0 0 2 2 0 2 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Check symplectic ok\n",
      "% [[32,2]], 96 checks, 18 iter ,trained\n",
      "\n",
      "% collect 300 frame errors or 45000000 decoded error patterns\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ment_lc\\Desktop\\quantum2\\prova.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\ment_lc\\Desktop\\quantum2\\prova.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m code \u001b[39m=\u001b[39m StabilizerCodes(n, k, m, codeType, matrix_supplier, trained)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m code\u001b[39m.\u001b[39madd_error_given_epsilon(epsilon)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m success \u001b[39m=\u001b[39m code\u001b[39m.\u001b[39mdecode(decIterNum, ep0)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m success[\u001b[39m1\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     failure \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\ment_lc\\Desktop\\quantum2\\prova.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_syndrome()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflooding_decode(L, epsilon)\n",
      "\u001b[1;32mc:\\Users\\ment_lc\\Desktop\\quantum2\\prova.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misnan(mv2c[Vidx][jj]):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mflooding_decode: mv2c[Vidx][jj] is NaN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misinf(mv2c[Vidx][jj]):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mflooding_decode: mv2c[Vidx][jj] is infinity\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ment_lc/Desktop/quantum2/prova.ipynb#W4sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m success \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_success(Taux, Tauy, Tauz)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import helpers\n",
    "\n",
    "def main():\n",
    "    n = 32\n",
    "    k = 2\n",
    "    m = 96\n",
    "    trained_iter=18\n",
    "    decIterNum = 18\n",
    "    trained = True\n",
    "    ep0 = 0.37\n",
    "    codeType = \"toric\"\n",
    "    print(\"PROVA1\")\n",
    "    matrix_supplier = FileReader(n, k, m, codeType, trained)\n",
    "    matrix_supplier.save_values_to_file()\n",
    "    matrix_supplier.check_symplectic()\n",
    "\n",
    "    default_max_frame_errors = 300\n",
    "    default_max_decoded_words = 45000000\n",
    "    default_ep_list = [\n",
    "        0.15, 0.135, 0.12, 0.105, 0.09, 0.075, 0.06, 0.045, 0.03, 0.015\n",
    "    ]\n",
    "\n",
    "    arguments = parse_arguments(argument_count=0, arguments=[], default_epsilons=default_ep_list,\n",
    "                                    maximum_frame_errors=default_max_frame_errors,\n",
    "                                    maximum_decoded_words=default_max_decoded_words)\n",
    "\n",
    "\n",
    "    ep_list = arguments.epsilons\n",
    "    max_frame_errors = arguments.maximum_frame_errors\n",
    "    max_decoded_words = arguments.maximum_decoded_words\n",
    "\n",
    "    print(f\"% [[{n},{k}]], {m} checks, {decIterNum} iter \", end=\"\")\n",
    "    if trained:\n",
    "        print(\",trained\")\n",
    "    print(\"\\n% collect\", max_frame_errors, \"frame errors or\", max_decoded_words, \"decoded error patterns\")\n",
    "\n",
    "    total_decoding = 0\n",
    "    failure = 0\n",
    "\n",
    "    for epsilon in ep_list:\n",
    "        while failure <= max_frame_errors and total_decoding <= max_decoded_words:\n",
    "            code = StabilizerCodes(n, k, m, codeType, matrix_supplier, trained)\n",
    "            code.add_error_given_epsilon(epsilon)\n",
    "            success = code.decode(decIterNum, ep0)\n",
    "            \n",
    "            if not success[1]:\n",
    "                failure += 1\n",
    "            total_decoding += 1\n",
    "\n",
    "        print(\"% FE\", failure, \", total dec.\", total_decoding, \"\\\\\\\\\")\n",
    "        print(epsilon, failure / total_decoding, \"\\\\\\\\\")\n",
    "        total_decoding = 0\n",
    "        failure = 0\n",
    "\n",
    "    while True:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix_supplier.compare_Mck_with_file(\"Mck.txt\"))\n",
    "print(matrix_supplier.compare_Mc_with_file(\"Mc.txt\"))\n",
    " print(matrix_supplier.compare_checkVal_with_file(\"checkVal.txt\"))\n",
    "    print(matrix_supplier.compare_varVal_with_file(\"varVal.txt\"))\n",
    "    print(matrix_supplier.compare_Nv_with_file(\"Nv.txt\"))\n",
    "    print(matrix_supplier.compare_Nvk_with_file(\"Nvk.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"maxDv:\", self.maxDv)\n",
    "print(\"maxDc:\", self.maxDc)\n",
    "print(\"dv:\", self.dv)\n",
    "print(\"dc:\", self.dc)\n",
    "print(\"Nvk:\", self.Nvk)\n",
    "print(\"Mck:\", self.Mck)\n",
    "print(\"Nv:\", self.Nv)\n",
    "print(\"Mc:\", self.Mc)\n",
    "print(\"checkVal:\", self.checkVal)\n",
    " print(\"varVal:\", self.varVal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
